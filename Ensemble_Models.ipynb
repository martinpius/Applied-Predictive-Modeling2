{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMvKn27TM5E9nWzDUlwamog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Applied-Predictive-Modeling2/blob/master/Ensemble_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNwOXmS5B6YS"
      },
      "source": [
        "#Print the correct tensorflow version "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy9-AcPXCiTa",
        "outputId": "330084f8-46d1-49f6-ed04-17488e6fd9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "  drive.mount('/content/drive/',force_remount = True)\n",
        "  COLAB = True\n",
        "  print(\"Google colab is on\")\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  COLAB = False\n",
        "  print(\"Not using Google Colab\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Google colab is on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8st7ZYrEPjZ"
      },
      "source": [
        "#Time formatter\n",
        "def time_fmt(secs):\n",
        "  hours = int(secs/(60*60))\n",
        "  minutes = int(secs%(60*60)/60)\n",
        "  seconds = int(secs%60)\n",
        "  return f\"{hours}:{minutes:>02}:{seconds:>05.2f}\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEN4Gj5CFKCr"
      },
      "source": [
        "#Pertubation implementation to variable importance for NNT"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9rYndBhIomj"
      },
      "source": [
        "import scipy as sc\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import math\n",
        "import pandas as pd\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_myYYLgdMf1s"
      },
      "source": [
        "#This function will compute the loss for a regression and classification model\n",
        "#To provide usefull information in computing variable's importance\n",
        "#First we select all columns from a test dataset\n",
        "#Second we randomly shuffles every columns to distort any parttern\n",
        "#If the fitted model is a regression we compute MSE otherwise we compute log-loss\n",
        "#We compute variable importance by take the ratio of individual error vs maximum error\n",
        "#We prepare a results dictionary which is finaly conveted to a dataframe."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msA56q1aJBzV"
      },
      "source": [
        "def p_rank(model, x, y, names, regression):\n",
        "  errors = []\n",
        "  for i in range(x.shape[1]):\n",
        "    hold = np.array(x[:,i])\n",
        "    np.random.shuffle(x[:,i])\n",
        "\n",
        "    if regression:\n",
        "      preds = model.predict(x)\n",
        "      err = metrics.mean_squared_error(preds, y)\n",
        "    else:\n",
        "      preds = model.predict_proba(x)\n",
        "      err = metrics.log_loss(preds, y)\n",
        "    errors.append(err)\n",
        "    x[:,i] = hold\n",
        "\n",
        "  max_error = np.max(errors)\n",
        "  var_importance = [(k/max_error) for k in errors]\n",
        "  data = {'Name':names, \"Errors\": errors, \"Var_importance\": var_importance }\n",
        "  outputs = pd.DataFrame(data, columns = ['Names', 'Errors', 'Variable Importance'])\n",
        "  outputs.sort_values(by = ['Variable Importance'], ascending = [0], inplace = True)\n",
        "  outputs.reset_index(inplace = True, drop = True)\n",
        "  return outputs\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HAXyp2ENzv8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}