{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian Optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNs9yItTY39k0QT4CtcA/2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Applied-Predictive-Modeling2/blob/master/Bayesian_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmunLHVtRC-n",
        "outputId": "6fd9b58d-f4bd-427c-b815-cf305149955c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "try: \n",
        "  drive.mount('/content/drive/', force_remount = True)\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are on Google Colab with tensorflow version {tf.__version__}\")\n",
        "except:\n",
        "  COLAB = False\n",
        "  print(\"You are not connected\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "You are on Google Colab with tensorflow version 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rg1un5pTweN"
      },
      "source": [
        "def timeset(x):\n",
        "  hrs = int(x/(60 *60))\n",
        "  mnt = int(x%(60 *60)/60)\n",
        "  secn = int(x%60)\n",
        "  return f\"{hrs}: {mnt:>02}: {secn:>05.2f}\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vXe6COeV3ra"
      },
      "source": [
        "import time\n",
        "import statistics\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from scipy.stats import zscore\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, PReLU, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY-0Ve2NXnPd"
      },
      "source": [
        "emp = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\", na_values = ['NAN','?'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "484qkfpmYCBa",
        "outputId": "4a997236-835b-4b3a-dbb3-46bfe781b8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "display(emp.head())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>job</th>\n",
              "      <th>area</th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>vv</td>\n",
              "      <td>c</td>\n",
              "      <td>50876.0</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>9.017895</td>\n",
              "      <td>35</td>\n",
              "      <td>11.738935</td>\n",
              "      <td>49</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.492126</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>kd</td>\n",
              "      <td>c</td>\n",
              "      <td>60369.0</td>\n",
              "      <td>18.625000</td>\n",
              "      <td>2</td>\n",
              "      <td>7.766643</td>\n",
              "      <td>59</td>\n",
              "      <td>6.805396</td>\n",
              "      <td>51</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.342520</td>\n",
              "      <td>0.400809</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pe</td>\n",
              "      <td>c</td>\n",
              "      <td>55126.0</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.632069</td>\n",
              "      <td>6</td>\n",
              "      <td>13.671772</td>\n",
              "      <td>44</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.207723</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>c</td>\n",
              "      <td>51690.0</td>\n",
              "      <td>15.808333</td>\n",
              "      <td>1</td>\n",
              "      <td>5.372942</td>\n",
              "      <td>16</td>\n",
              "      <td>4.333286</td>\n",
              "      <td>50</td>\n",
              "      <td>0.889764</td>\n",
              "      <td>0.444882</td>\n",
              "      <td>0.361216</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>kl</td>\n",
              "      <td>d</td>\n",
              "      <td>28347.0</td>\n",
              "      <td>40.941667</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>20</td>\n",
              "      <td>5.967121</td>\n",
              "      <td>38</td>\n",
              "      <td>0.744094</td>\n",
              "      <td>0.661417</td>\n",
              "      <td>0.068033</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id job area   income  ...  pop_dense  retail_dense     crime  product\n",
              "0   1  vv    c  50876.0  ...   0.885827      0.492126  0.071100        b\n",
              "1   2  kd    c  60369.0  ...   0.874016      0.342520  0.400809        c\n",
              "2   3  pe    c  55126.0  ...   0.944882      0.724409  0.207723        b\n",
              "3   4  11    c  51690.0  ...   0.889764      0.444882  0.361216        b\n",
              "4   5  kl    d  28347.0  ...   0.744094      0.661417  0.068033        a\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px0udZWeYLyX"
      },
      "source": [
        "emp.drop('id', axis = 1, inplace = True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GbeBdP2YSEo"
      },
      "source": [
        "emp['income'] = emp['income'].fillna(emp['income'].median())"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXX0fYkCYlFd"
      },
      "source": [
        "contcols = emp.columns.drop(['job','area','product','subscriptions'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1KQ-A0FZIWz"
      },
      "source": [
        "y = emp['product']"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kiw-XaerZPG-"
      },
      "source": [
        "emp.drop('product', axis = 1, inplace = True)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rasbucnMZYv7"
      },
      "source": [
        "aread = pd.get_dummies(emp['area'], prefix = 'area')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVlifUMqZfwi"
      },
      "source": [
        "jobd = pd.get_dummies(emp['job'], prefix = 'job')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Z419EiZnFB"
      },
      "source": [
        "emp.drop(['job', 'area'], axis = 1, inplace = True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO5HBgfLZu9D",
        "outputId": "1adb90d4-ee61-4198-f93c-1fc1a17a0ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "display(emp.head())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50876.0</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>9.017895</td>\n",
              "      <td>35</td>\n",
              "      <td>11.738935</td>\n",
              "      <td>49</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.492126</td>\n",
              "      <td>0.071100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60369.0</td>\n",
              "      <td>18.625000</td>\n",
              "      <td>2</td>\n",
              "      <td>7.766643</td>\n",
              "      <td>59</td>\n",
              "      <td>6.805396</td>\n",
              "      <td>51</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.342520</td>\n",
              "      <td>0.400809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>55126.0</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.632069</td>\n",
              "      <td>6</td>\n",
              "      <td>13.671772</td>\n",
              "      <td>44</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.207723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51690.0</td>\n",
              "      <td>15.808333</td>\n",
              "      <td>1</td>\n",
              "      <td>5.372942</td>\n",
              "      <td>16</td>\n",
              "      <td>4.333286</td>\n",
              "      <td>50</td>\n",
              "      <td>0.889764</td>\n",
              "      <td>0.444882</td>\n",
              "      <td>0.361216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28347.0</td>\n",
              "      <td>40.941667</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>20</td>\n",
              "      <td>5.967121</td>\n",
              "      <td>38</td>\n",
              "      <td>0.744094</td>\n",
              "      <td>0.661417</td>\n",
              "      <td>0.068033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    income     aspect  subscriptions  ...  pop_dense  retail_dense     crime\n",
              "0  50876.0  13.100000              1  ...   0.885827      0.492126  0.071100\n",
              "1  60369.0  18.625000              2  ...   0.874016      0.342520  0.400809\n",
              "2  55126.0  34.766667              1  ...   0.944882      0.724409  0.207723\n",
              "3  51690.0  15.808333              1  ...   0.889764      0.444882  0.361216\n",
              "4  28347.0  40.941667              3  ...   0.744094      0.661417  0.068033\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc4oS0PIZx4b"
      },
      "source": [
        "for col in contcols:\n",
        "  emp[col] = zscore(emp[col])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A_xtEdyZ6jE",
        "outputId": "181f244f-72f9-4a75-ed4c-50fafe77e9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "display(emp.head())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.607550</td>\n",
              "      <td>-0.664918</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.048411</td>\n",
              "      <td>-0.215764</td>\n",
              "      <td>-0.314089</td>\n",
              "      <td>0.854321</td>\n",
              "      <td>0.079279</td>\n",
              "      <td>-0.465765</td>\n",
              "      <td>-1.120315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.338053</td>\n",
              "      <td>-0.207748</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.266765</td>\n",
              "      <td>0.196869</td>\n",
              "      <td>-0.915161</td>\n",
              "      <td>1.394432</td>\n",
              "      <td>-0.075010</td>\n",
              "      <td>-1.445372</td>\n",
              "      <td>0.682945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.184205</td>\n",
              "      <td>1.127906</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.988286</td>\n",
              "      <td>-0.714362</td>\n",
              "      <td>-0.078604</td>\n",
              "      <td>-0.495957</td>\n",
              "      <td>0.850727</td>\n",
              "      <td>1.055205</td>\n",
              "      <td>-0.373087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.526467</td>\n",
              "      <td>-0.440815</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.684488</td>\n",
              "      <td>-0.542432</td>\n",
              "      <td>-1.216347</td>\n",
              "      <td>1.124377</td>\n",
              "      <td>0.130709</td>\n",
              "      <td>-0.775115</td>\n",
              "      <td>0.466401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.851675</td>\n",
              "      <td>1.638861</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.955058</td>\n",
              "      <td>-0.473660</td>\n",
              "      <td>-1.017291</td>\n",
              "      <td>-2.116291</td>\n",
              "      <td>-1.772196</td>\n",
              "      <td>0.642739</td>\n",
              "      <td>-1.137090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     income    aspect  subscriptions  ...  pop_dense  retail_dense     crime\n",
              "0 -0.607550 -0.664918              1  ...   0.079279     -0.465765 -1.120315\n",
              "1  0.338053 -0.207748              2  ...  -0.075010     -1.445372  0.682945\n",
              "2 -0.184205  1.127906              1  ...   0.850727      1.055205 -0.373087\n",
              "3 -0.526467 -0.440815              1  ...   0.130709     -0.775115  0.466401\n",
              "4 -2.851675  1.638861              3  ...  -1.772196      0.642739 -1.137090\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKQFBrkGZ9sv"
      },
      "source": [
        "emp = pd.concat([emp, aread, jobd], axis = 1)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k2mhI9FaLBL",
        "outputId": "2dab2540-c072-485f-b6eb-792e2d1d77de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "display(emp.head())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>area_a</th>\n",
              "      <th>area_b</th>\n",
              "      <th>area_c</th>\n",
              "      <th>area_d</th>\n",
              "      <th>job_11</th>\n",
              "      <th>job_al</th>\n",
              "      <th>job_am</th>\n",
              "      <th>job_ax</th>\n",
              "      <th>job_bf</th>\n",
              "      <th>job_by</th>\n",
              "      <th>job_cv</th>\n",
              "      <th>job_de</th>\n",
              "      <th>job_dz</th>\n",
              "      <th>job_e2</th>\n",
              "      <th>job_f8</th>\n",
              "      <th>job_gj</th>\n",
              "      <th>job_gv</th>\n",
              "      <th>job_kd</th>\n",
              "      <th>job_ke</th>\n",
              "      <th>job_kl</th>\n",
              "      <th>job_kp</th>\n",
              "      <th>job_ks</th>\n",
              "      <th>job_kw</th>\n",
              "      <th>job_mm</th>\n",
              "      <th>job_nb</th>\n",
              "      <th>job_nn</th>\n",
              "      <th>job_ob</th>\n",
              "      <th>job_pe</th>\n",
              "      <th>job_po</th>\n",
              "      <th>job_pq</th>\n",
              "      <th>job_pz</th>\n",
              "      <th>job_qp</th>\n",
              "      <th>job_qw</th>\n",
              "      <th>job_rn</th>\n",
              "      <th>job_sa</th>\n",
              "      <th>job_vv</th>\n",
              "      <th>job_zz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.607550</td>\n",
              "      <td>-0.664918</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.048411</td>\n",
              "      <td>-0.215764</td>\n",
              "      <td>-0.314089</td>\n",
              "      <td>0.854321</td>\n",
              "      <td>0.079279</td>\n",
              "      <td>-0.465765</td>\n",
              "      <td>-1.120315</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.338053</td>\n",
              "      <td>-0.207748</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.266765</td>\n",
              "      <td>0.196869</td>\n",
              "      <td>-0.915161</td>\n",
              "      <td>1.394432</td>\n",
              "      <td>-0.075010</td>\n",
              "      <td>-1.445372</td>\n",
              "      <td>0.682945</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.184205</td>\n",
              "      <td>1.127906</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.988286</td>\n",
              "      <td>-0.714362</td>\n",
              "      <td>-0.078604</td>\n",
              "      <td>-0.495957</td>\n",
              "      <td>0.850727</td>\n",
              "      <td>1.055205</td>\n",
              "      <td>-0.373087</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.526467</td>\n",
              "      <td>-0.440815</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.684488</td>\n",
              "      <td>-0.542432</td>\n",
              "      <td>-1.216347</td>\n",
              "      <td>1.124377</td>\n",
              "      <td>0.130709</td>\n",
              "      <td>-0.775115</td>\n",
              "      <td>0.466401</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.851675</td>\n",
              "      <td>1.638861</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.955058</td>\n",
              "      <td>-0.473660</td>\n",
              "      <td>-1.017291</td>\n",
              "      <td>-2.116291</td>\n",
              "      <td>-1.772196</td>\n",
              "      <td>0.642739</td>\n",
              "      <td>-1.137090</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     income    aspect  subscriptions  ...  job_sa  job_vv  job_zz\n",
              "0 -0.607550 -0.664918              1  ...       0       1       0\n",
              "1  0.338053 -0.207748              2  ...       0       0       0\n",
              "2 -0.184205  1.127906              1  ...       0       0       0\n",
              "3 -0.526467 -0.440815              1  ...       0       0       0\n",
              "4 -2.851675  1.638861              3  ...       0       0       0\n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyt9leWyaOr5"
      },
      "source": [
        "x = emp.values"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytrBxyBaSDO"
      },
      "source": [
        "y = pd.get_dummies(y).values"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb3VVY4waZqw",
        "outputId": "b57c5fea-3b86-4c76-f286-95d51141742c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x[0:10]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.60754957, -0.66491815,  1.        , -0.04841068, -0.21576413,\n",
              "        -0.31408854,  0.85432106,  0.07927915, -0.46576475, -1.12031509,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         1.        ,  0.        ],\n",
              "       [ 0.33805295, -0.20774798,  2.        , -0.26676549,  0.19686897,\n",
              "        -0.91516076,  1.39443237, -0.07501047, -1.44537236,  0.68294487,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.18420492,  1.12790602,  1.        , -0.98828572, -0.71436245,\n",
              "        -0.07860353, -0.49595721,  0.85072725,  1.05520496, -0.37308686,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.52646662, -0.44081512,  1.        , -0.68448773, -0.54243199,\n",
              "        -1.21634739,  1.12437672,  0.13070902, -0.77511452,  0.46640118,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-2.85167463,  1.63886091,  3.        , -0.95505781, -0.47365981,\n",
              "        -1.017291  , -2.11629114, -1.77219629,  0.6427386 , -1.1370896 ,\n",
              "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.38246909,  1.59404031,  1.        ,  0.97690753,  0.67827425,\n",
              "         0.7338848 , -0.76601287, -0.17787022,  0.72007604,  1.08095827,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-1.8178172 ,  0.81416179,  3.        , -0.95505781, -0.25015022,\n",
              "        -0.58925472, -1.84623548,  1.26216624,  2.03481257, -1.00518094,\n",
              "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.18061895,  0.48248932,  2.        , -0.86961463, -0.52523895,\n",
              "         1.8156847 , -0.49595721,  1.57074548,  1.05520496, -0.61860471,\n",
              "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 1.02954907,  0.93069536,  0.        ,  2.75697388,  2.08810399,\n",
              "        -0.41296622, -0.22590156,  0.953587  ,  0.77163434, -0.98230661,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.6343942 ,  1.41475789,  1.        , -1.13069102, -0.76594159,\n",
              "         0.92578556, -1.03606852,  0.23356877,  1.05520496, -0.55760648,\n",
              "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfA5L_YoacDr",
        "outputId": "915307cc-b8a7-438e-e359-28bcdd184884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y[0:10]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKQXUXW9aeMq"
      },
      "source": [
        "#Model setup"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK7rblFfahKe",
        "outputId": "870d9524-eb68-42f4-8d13-6a13c5d92292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def my_turned_model(dropout, lr,neuronRate, neuronDrop):\n",
        "  SPLITS = 10\n",
        "  boots_samples = StratifiedShuffleSplit(n_splits = SPLITS, test_size= 0.1)\n",
        "  average_epoch = []\n",
        "  average_lloss = []\n",
        "  count = 0\n",
        "  no_neurons = (neuronRate * 8000)\n",
        "  \n",
        "  #Looping over the bootstraps samples\n",
        "  for train, test in boots_samples.split(x, y):\n",
        "    begin1 = time.time()\n",
        "    count+=1\n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    #Model setup\n",
        "    layers = 0\n",
        "    model = Sequential()\n",
        "    while layers < 10 and no_neurons > 40:\n",
        "      if layers == 0:\n",
        "        model.add(Dense(no_neurons, input_dim = x.shape[1], activation = PReLU(), kernel_initializer = 'random_normal'))\n",
        "      else:\n",
        "        model.add(Dense(no_neurons, activation = PReLU()))\n",
        "      no_neurons = (no_neurons * neuronRate)\n",
        "      model.add(Dropout(dropout))\n",
        "      model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "      model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr))\n",
        "      info = EarlyStopping(monitor = 'val_loss', patience = 100, verbose = 1, min_delta = 1e-3, restore_best_weights = True)\n",
        "      model.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = True, epochs = 1000, callbacks = [info])\n",
        "      epoc = info.stopped_epoch\n",
        "      average_epoch.append(epoc)\n",
        "      pred = model.predict(x_test)\n",
        "      y_match = np.argmax(y_test, axis = 1)\n",
        "      scoreloss = metrics.log_loss(y_test, pred)\n",
        "      average_lloss.append(scoreloss)\n",
        "      mean1 = statistics.mean(average_epoch)\n",
        "      mean2 = statistics.mean(average_lloss)\n",
        "      mean_dev = statistics.pstdev(average_lloss)\n",
        "    end1 = time.time()\n",
        "    total_time = begin1 - end1\n",
        "    total_time = timeset(total_time)\n",
        "    tensorflow.keras.backend.clear_session()\n",
        "    return f\"Minus Mean_score_logloss: {mean2}\\n Average Epoch :{mean1}\\n Mean_deviation_loss: {mean_dev}, Time elapsed: {total_time}\"\n",
        "print(my_turned_model(dropout = 0.25,lr = 1e-03,neuronRate = 0.2, neuronDrop = 0.25))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1.0376 - val_loss: 0.7584\n",
            "Epoch 2/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.7589 - val_loss: 0.6823\n",
            "Epoch 3/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6883 - val_loss: 0.6655\n",
            "Epoch 4/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6469 - val_loss: 0.6429\n",
            "Epoch 5/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6250 - val_loss: 0.6070\n",
            "Epoch 6/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6092 - val_loss: 0.6262\n",
            "Epoch 7/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5836 - val_loss: 0.6043\n",
            "Epoch 8/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5708 - val_loss: 0.6026\n",
            "Epoch 9/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5646 - val_loss: 0.6143\n",
            "Epoch 10/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5375 - val_loss: 0.6152\n",
            "Epoch 11/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.6216\n",
            "Epoch 12/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5146 - val_loss: 0.6046\n",
            "Epoch 13/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4985 - val_loss: 0.6064\n",
            "Epoch 14/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4863 - val_loss: 0.5888\n",
            "Epoch 15/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4746 - val_loss: 0.6175\n",
            "Epoch 16/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4599 - val_loss: 0.6069\n",
            "Epoch 17/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4600 - val_loss: 0.5979\n",
            "Epoch 18/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4434 - val_loss: 0.6105\n",
            "Epoch 19/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4381 - val_loss: 0.6161\n",
            "Epoch 20/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4290 - val_loss: 0.6088\n",
            "Epoch 21/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4110 - val_loss: 0.6146\n",
            "Epoch 22/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4107 - val_loss: 0.6230\n",
            "Epoch 23/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3921 - val_loss: 0.6191\n",
            "Epoch 24/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3849 - val_loss: 0.6343\n",
            "Epoch 25/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.6475\n",
            "Epoch 26/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3690 - val_loss: 0.6490\n",
            "Epoch 27/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3703 - val_loss: 0.6466\n",
            "Epoch 28/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 0.6543\n",
            "Epoch 29/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 0.6553\n",
            "Epoch 30/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3383 - val_loss: 0.6448\n",
            "Epoch 31/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.6605\n",
            "Epoch 32/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 0.6526\n",
            "Epoch 33/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3183 - val_loss: 0.6533\n",
            "Epoch 34/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3159 - val_loss: 0.6546\n",
            "Epoch 35/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3075 - val_loss: 0.6973\n",
            "Epoch 36/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 0.6620\n",
            "Epoch 37/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2958 - val_loss: 0.6745\n",
            "Epoch 38/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2990 - val_loss: 0.6805\n",
            "Epoch 39/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2862 - val_loss: 0.6851\n",
            "Epoch 40/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2828 - val_loss: 0.6903\n",
            "Epoch 41/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.6942\n",
            "Epoch 42/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2773 - val_loss: 0.7042\n",
            "Epoch 43/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2811 - val_loss: 0.6890\n",
            "Epoch 44/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 0.7503\n",
            "Epoch 45/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2727 - val_loss: 0.6890\n",
            "Epoch 46/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2662 - val_loss: 0.7258\n",
            "Epoch 47/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2555 - val_loss: 0.7107\n",
            "Epoch 48/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2566 - val_loss: 0.7183\n",
            "Epoch 49/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2423 - val_loss: 0.7218\n",
            "Epoch 50/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2470 - val_loss: 0.7382\n",
            "Epoch 51/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2379 - val_loss: 0.7269\n",
            "Epoch 52/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2280 - val_loss: 0.7078\n",
            "Epoch 53/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.7565\n",
            "Epoch 54/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 0.7629\n",
            "Epoch 55/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2294 - val_loss: 0.7838\n",
            "Epoch 56/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2549 - val_loss: 0.7759\n",
            "Epoch 57/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2328 - val_loss: 0.7554\n",
            "Epoch 58/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2216 - val_loss: 0.7743\n",
            "Epoch 59/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2215 - val_loss: 0.8168\n",
            "Epoch 60/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2166 - val_loss: 0.7788\n",
            "Epoch 61/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1986 - val_loss: 0.7708\n",
            "Epoch 62/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2040 - val_loss: 0.7957\n",
            "Epoch 63/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1992 - val_loss: 0.8345\n",
            "Epoch 64/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1894 - val_loss: 0.7665\n",
            "Epoch 65/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2020 - val_loss: 0.8002\n",
            "Epoch 66/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1913 - val_loss: 0.8047\n",
            "Epoch 67/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2019 - val_loss: 0.8020\n",
            "Epoch 68/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.7993\n",
            "Epoch 69/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.8137\n",
            "Epoch 70/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2034 - val_loss: 0.8317\n",
            "Epoch 71/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1884 - val_loss: 0.8173\n",
            "Epoch 72/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1974 - val_loss: 0.8004\n",
            "Epoch 73/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1761 - val_loss: 0.8875\n",
            "Epoch 74/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1773 - val_loss: 0.8377\n",
            "Epoch 75/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 0.8632\n",
            "Epoch 76/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 0.8596\n",
            "Epoch 77/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 0.8678\n",
            "Epoch 78/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 0.8504\n",
            "Epoch 79/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2165 - val_loss: 0.8220\n",
            "Epoch 80/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.8452\n",
            "Epoch 81/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1711 - val_loss: 0.8371\n",
            "Epoch 82/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1716 - val_loss: 0.8828\n",
            "Epoch 83/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1624 - val_loss: 0.8502\n",
            "Epoch 84/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1723 - val_loss: 0.9085\n",
            "Epoch 85/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.8861\n",
            "Epoch 86/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1679 - val_loss: 0.8869\n",
            "Epoch 87/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.9423\n",
            "Epoch 88/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1617 - val_loss: 0.8935\n",
            "Epoch 89/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1593 - val_loss: 0.9044\n",
            "Epoch 90/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1540 - val_loss: 0.9512\n",
            "Epoch 91/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 0.9177\n",
            "Epoch 92/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1595 - val_loss: 0.9894\n",
            "Epoch 93/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1525 - val_loss: 0.9391\n",
            "Epoch 94/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 0.9949\n",
            "Epoch 95/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1523 - val_loss: 0.9660\n",
            "Epoch 96/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1640 - val_loss: 0.9537\n",
            "Epoch 97/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1460 - val_loss: 0.9585\n",
            "Epoch 98/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.9339\n",
            "Epoch 99/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1622 - val_loss: 0.9560\n",
            "Epoch 100/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1453 - val_loss: 0.9345\n",
            "Epoch 101/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 0.9379\n",
            "Epoch 102/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1612 - val_loss: 0.9529\n",
            "Epoch 103/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1456 - val_loss: 0.9758\n",
            "Epoch 104/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 0.9763\n",
            "Epoch 105/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1330 - val_loss: 0.9953\n",
            "Epoch 106/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.9978\n",
            "Epoch 107/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.9546\n",
            "Epoch 108/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.9823\n",
            "Epoch 109/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 1.0114\n",
            "Epoch 110/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1466 - val_loss: 0.9682\n",
            "Epoch 111/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1431 - val_loss: 1.0077\n",
            "Epoch 112/1000\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 0.9451\n",
            "Epoch 113/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.9794\n",
            "Epoch 114/1000\n",
            "45/57 [======================>.......] - ETA: 0s - loss: 0.1479Restoring model weights from the end of the best epoch.\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 1.0223\n",
            "Epoch 00114: early stopping\n",
            "Epoch 1/1000\n",
            "57/57 [==============================] - 0s 5ms/step - loss: 1.5036 - val_loss: 1.0987\n",
            "Epoch 2/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.8741 - val_loss: 0.7534\n",
            "Epoch 3/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6448 - val_loss: 0.6965\n",
            "Epoch 4/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5725 - val_loss: 0.6951\n",
            "Epoch 5/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 0.6728\n",
            "Epoch 6/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4926 - val_loss: 0.6876\n",
            "Epoch 7/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4610 - val_loss: 0.7159\n",
            "Epoch 8/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4539 - val_loss: 0.6881\n",
            "Epoch 9/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4540 - val_loss: 0.7082\n",
            "Epoch 10/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 0.7369\n",
            "Epoch 11/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 0.7269\n",
            "Epoch 12/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3880 - val_loss: 0.7430\n",
            "Epoch 13/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3790 - val_loss: 0.7185\n",
            "Epoch 14/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3504 - val_loss: 0.7214\n",
            "Epoch 15/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3659 - val_loss: 0.8305\n",
            "Epoch 16/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3400 - val_loss: 0.7491\n",
            "Epoch 17/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3425 - val_loss: 0.7815\n",
            "Epoch 18/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3601 - val_loss: 0.7343\n",
            "Epoch 19/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3235 - val_loss: 0.7863\n",
            "Epoch 20/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 0.7673\n",
            "Epoch 21/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 0.7347\n",
            "Epoch 22/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 0.7854\n",
            "Epoch 23/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3036 - val_loss: 0.7845\n",
            "Epoch 24/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 0.8249\n",
            "Epoch 25/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3236 - val_loss: 0.8447\n",
            "Epoch 26/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2922 - val_loss: 0.8252\n",
            "Epoch 27/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 0.8345\n",
            "Epoch 28/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 0.8365\n",
            "Epoch 29/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2845 - val_loss: 0.8223\n",
            "Epoch 30/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 0.8693\n",
            "Epoch 31/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2625 - val_loss: 0.8732\n",
            "Epoch 32/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2696 - val_loss: 0.8969\n",
            "Epoch 33/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 0.8535\n",
            "Epoch 34/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2515 - val_loss: 0.8719\n",
            "Epoch 35/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2548 - val_loss: 0.8475\n",
            "Epoch 36/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2619 - val_loss: 0.9284\n",
            "Epoch 37/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 0.8835\n",
            "Epoch 38/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2527 - val_loss: 0.9868\n",
            "Epoch 39/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2765 - val_loss: 0.8649\n",
            "Epoch 40/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2329 - val_loss: 0.8924\n",
            "Epoch 41/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2333 - val_loss: 0.8978\n",
            "Epoch 42/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 0.9148\n",
            "Epoch 43/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2505 - val_loss: 0.9084\n",
            "Epoch 44/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2439 - val_loss: 0.9736\n",
            "Epoch 45/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2663 - val_loss: 0.9503\n",
            "Epoch 46/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2253 - val_loss: 0.9669\n",
            "Epoch 47/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2261 - val_loss: 0.9518\n",
            "Epoch 48/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2441 - val_loss: 0.8850\n",
            "Epoch 49/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 0.9286\n",
            "Epoch 50/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1938 - val_loss: 0.9574\n",
            "Epoch 51/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 1.0084\n",
            "Epoch 52/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 0.9531\n",
            "Epoch 53/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2153 - val_loss: 0.9598\n",
            "Epoch 54/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1949 - val_loss: 0.9528\n",
            "Epoch 55/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1987 - val_loss: 0.9863\n",
            "Epoch 56/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2283 - val_loss: 1.0030\n",
            "Epoch 57/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 0.9390\n",
            "Epoch 58/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1858 - val_loss: 1.0238\n",
            "Epoch 59/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1968 - val_loss: 1.0145\n",
            "Epoch 60/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2023 - val_loss: 0.9780\n",
            "Epoch 61/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1713 - val_loss: 0.9553\n",
            "Epoch 62/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 1.0022\n",
            "Epoch 63/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 1.0146\n",
            "Epoch 64/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 0.9752\n",
            "Epoch 65/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1939 - val_loss: 0.9858\n",
            "Epoch 66/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 1.0309\n",
            "Epoch 67/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1985 - val_loss: 0.9766\n",
            "Epoch 68/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1767 - val_loss: 1.0550\n",
            "Epoch 69/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 1.0730\n",
            "Epoch 70/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1866 - val_loss: 1.0672\n",
            "Epoch 71/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1999 - val_loss: 1.0080\n",
            "Epoch 72/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1901 - val_loss: 1.0379\n",
            "Epoch 73/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 1.1017\n",
            "Epoch 74/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1788 - val_loss: 1.0037\n",
            "Epoch 75/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1674 - val_loss: 1.0248\n",
            "Epoch 76/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1537 - val_loss: 1.1126\n",
            "Epoch 77/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1750 - val_loss: 1.0593\n",
            "Epoch 78/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 1.1318\n",
            "Epoch 79/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 1.0595\n",
            "Epoch 80/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1905 - val_loss: 1.0764\n",
            "Epoch 81/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 1.0801\n",
            "Epoch 82/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1608 - val_loss: 1.0672\n",
            "Epoch 83/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 1.0844\n",
            "Epoch 84/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1556 - val_loss: 1.1663\n",
            "Epoch 85/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1378 - val_loss: 1.0601\n",
            "Epoch 86/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1466 - val_loss: 1.0594\n",
            "Epoch 87/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1537 - val_loss: 1.0691\n",
            "Epoch 88/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1805 - val_loss: 1.0464\n",
            "Epoch 89/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1522 - val_loss: 1.0968\n",
            "Epoch 90/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1744 - val_loss: 1.2341\n",
            "Epoch 91/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1660 - val_loss: 1.1470\n",
            "Epoch 92/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 1.1281\n",
            "Epoch 93/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1496 - val_loss: 1.0961\n",
            "Epoch 94/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1513 - val_loss: 1.1660\n",
            "Epoch 95/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1472 - val_loss: 1.0943\n",
            "Epoch 96/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1654 - val_loss: 1.1130\n",
            "Epoch 97/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 1.1993\n",
            "Epoch 98/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1421 - val_loss: 1.1887\n",
            "Epoch 99/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 1.1458\n",
            "Epoch 100/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 1.1453\n",
            "Epoch 101/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1345 - val_loss: 1.1862\n",
            "Epoch 102/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 1.0985\n",
            "Epoch 103/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 1.1304\n",
            "Epoch 104/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1754 - val_loss: 1.0592\n",
            "Epoch 105/1000\n",
            "43/57 [=====================>........] - ETA: 0s - loss: 0.1492Restoring model weights from the end of the best epoch.\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1485 - val_loss: 1.0924\n",
            "Epoch 00105: early stopping\n",
            "Epoch 1/1000\n",
            "57/57 [==============================] - 0s 5ms/step - loss: 1.7644 - val_loss: 1.5708\n",
            "Epoch 2/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1.3473 - val_loss: 1.1552\n",
            "Epoch 3/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1.0300 - val_loss: 0.9852\n",
            "Epoch 4/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.9174 - val_loss: 0.9170\n",
            "Epoch 5/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.8098 - val_loss: 0.8122\n",
            "Epoch 6/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6959 - val_loss: 0.7511\n",
            "Epoch 7/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6373 - val_loss: 0.7787\n",
            "Epoch 8/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.6393 - val_loss: 0.7451\n",
            "Epoch 9/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5866 - val_loss: 0.7147\n",
            "Epoch 10/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5568 - val_loss: 0.7405\n",
            "Epoch 11/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5285 - val_loss: 0.7428\n",
            "Epoch 12/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5293 - val_loss: 0.7238\n",
            "Epoch 13/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5062 - val_loss: 0.7418\n",
            "Epoch 14/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5126 - val_loss: 0.7525\n",
            "Epoch 15/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4961 - val_loss: 0.7610\n",
            "Epoch 16/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4828 - val_loss: 0.7715\n",
            "Epoch 17/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4908 - val_loss: 0.7747\n",
            "Epoch 18/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4713 - val_loss: 0.7410\n",
            "Epoch 19/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4627 - val_loss: 0.7795\n",
            "Epoch 20/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4695 - val_loss: 0.7681\n",
            "Epoch 21/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4836 - val_loss: 0.8031\n",
            "Epoch 22/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4413 - val_loss: 0.7650\n",
            "Epoch 23/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4219 - val_loss: 0.7543\n",
            "Epoch 24/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4355 - val_loss: 0.7759\n",
            "Epoch 25/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4190 - val_loss: 0.7763\n",
            "Epoch 26/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4140 - val_loss: 0.7878\n",
            "Epoch 27/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3922 - val_loss: 0.7925\n",
            "Epoch 28/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4139 - val_loss: 0.7905\n",
            "Epoch 29/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4298 - val_loss: 0.7588\n",
            "Epoch 30/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4080 - val_loss: 0.7873\n",
            "Epoch 31/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3803 - val_loss: 0.7379\n",
            "Epoch 32/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3917 - val_loss: 0.7624\n",
            "Epoch 33/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3753 - val_loss: 0.8165\n",
            "Epoch 34/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 0.7983\n",
            "Epoch 35/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3695 - val_loss: 0.7927\n",
            "Epoch 36/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3671 - val_loss: 0.8109\n",
            "Epoch 37/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3654 - val_loss: 0.8120\n",
            "Epoch 38/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3557 - val_loss: 0.8320\n",
            "Epoch 39/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4033 - val_loss: 0.8135\n",
            "Epoch 40/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3740 - val_loss: 0.7816\n",
            "Epoch 41/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3452 - val_loss: 0.8051\n",
            "Epoch 42/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3531 - val_loss: 0.8460\n",
            "Epoch 43/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3212 - val_loss: 0.8775\n",
            "Epoch 44/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3139 - val_loss: 0.8823\n",
            "Epoch 45/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3432 - val_loss: 0.7897\n",
            "Epoch 46/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3205 - val_loss: 0.8020\n",
            "Epoch 47/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3153 - val_loss: 0.8474\n",
            "Epoch 48/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3129 - val_loss: 0.8719\n",
            "Epoch 49/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 0.8401\n",
            "Epoch 50/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3140 - val_loss: 0.9481\n",
            "Epoch 51/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3295 - val_loss: 0.8422\n",
            "Epoch 52/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 0.8550\n",
            "Epoch 53/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2898 - val_loss: 0.8790\n",
            "Epoch 54/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2669 - val_loss: 0.9084\n",
            "Epoch 55/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3186 - val_loss: 0.8720\n",
            "Epoch 56/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2876 - val_loss: 0.8921\n",
            "Epoch 57/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3127 - val_loss: 0.8949\n",
            "Epoch 58/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 0.8685\n",
            "Epoch 59/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2861 - val_loss: 0.8885\n",
            "Epoch 60/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2686 - val_loss: 0.8360\n",
            "Epoch 61/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 0.9239\n",
            "Epoch 62/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 0.8510\n",
            "Epoch 63/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3318 - val_loss: 0.8574\n",
            "Epoch 64/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3116 - val_loss: 0.8825\n",
            "Epoch 65/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2691 - val_loss: 0.8905\n",
            "Epoch 66/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2873 - val_loss: 0.8665\n",
            "Epoch 67/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 0.9068\n",
            "Epoch 68/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 0.8707\n",
            "Epoch 69/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 0.9011\n",
            "Epoch 70/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2617 - val_loss: 0.8564\n",
            "Epoch 71/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 0.9189\n",
            "Epoch 72/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2520 - val_loss: 0.9201\n",
            "Epoch 73/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2468 - val_loss: 0.8900\n",
            "Epoch 74/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2595 - val_loss: 0.8612\n",
            "Epoch 75/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2434 - val_loss: 0.9166\n",
            "Epoch 76/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2499 - val_loss: 0.9220\n",
            "Epoch 77/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 0.9767\n",
            "Epoch 78/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 0.9587\n",
            "Epoch 79/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2548 - val_loss: 0.9004\n",
            "Epoch 80/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2472 - val_loss: 0.9314\n",
            "Epoch 81/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2669 - val_loss: 0.9037\n",
            "Epoch 82/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 0.9513\n",
            "Epoch 83/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2598 - val_loss: 0.8899\n",
            "Epoch 84/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2298 - val_loss: 0.9195\n",
            "Epoch 85/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2364 - val_loss: 0.9342\n",
            "Epoch 86/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 1.0063\n",
            "Epoch 87/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2789 - val_loss: 0.9281\n",
            "Epoch 88/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2507 - val_loss: 0.9664\n",
            "Epoch 89/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2312 - val_loss: 0.9365\n",
            "Epoch 90/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.9901\n",
            "Epoch 91/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2579 - val_loss: 0.9258\n",
            "Epoch 92/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2397 - val_loss: 0.9583\n",
            "Epoch 93/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2356 - val_loss: 0.9384\n",
            "Epoch 94/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2336 - val_loss: 0.9593\n",
            "Epoch 95/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2329 - val_loss: 0.8661\n",
            "Epoch 96/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2510 - val_loss: 0.9782\n",
            "Epoch 97/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 0.9494\n",
            "Epoch 98/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 1.0007\n",
            "Epoch 99/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2499 - val_loss: 1.0138\n",
            "Epoch 100/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 0.8666\n",
            "Epoch 101/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2192 - val_loss: 0.9912\n",
            "Epoch 102/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 1.0486\n",
            "Epoch 103/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 0.9513\n",
            "Epoch 104/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2453 - val_loss: 0.9829\n",
            "Epoch 105/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2187 - val_loss: 0.9391\n",
            "Epoch 106/1000\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2128 - val_loss: 0.9292\n",
            "Epoch 107/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 0.9786\n",
            "Epoch 108/1000\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2092 - val_loss: 1.0359\n",
            "Epoch 109/1000\n",
            "54/57 [===========================>..] - ETA: 0s - loss: 0.2046Restoring model weights from the end of the best epoch.\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2092 - val_loss: 0.9931\n",
            "Epoch 00109: early stopping\n",
            "Minus Mean_score_logloss: 0.6587288609313934\n",
            " Average Epoch :108.33333333333333\n",
            " Mean_deviation_loss: 0.05235461391460276, Time elapsed: 0: 59: 01.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQWedmihigJu"
      },
      "source": [
        "#Model Hyperparameters turning's generator\n",
        "def helper_hyper(dropout, neuronpct,neuronCut):\n",
        "  num_neurons = (6000 * neuronpct)\n",
        "  layers = 0\n",
        "  model = Sequential()\n",
        "  while layers < 10 and num_neurons > 50:\n",
        "    if layers == 0:\n",
        "      model.add(Dense(num_neurons, input_dim = x.shape[1], activation = PReLU(), kernel_initializer = 'random_normal'))\n",
        "    else:\n",
        "      model.add(Dense(num_neurons, activation = PReLU(), kernel_initializer = 'random_normal'))\n",
        "    layers+=1\n",
        "    num_neurons = (num_neurons * neuronCut)\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "  return model"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEEBnklwmwPl"
      },
      "source": [
        "mymodel = helper_hyper(dropout = 0.20, neuronpct=0.1,neuronCut=0.20)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKmtDyCbnAP2",
        "outputId": "da055a05-522a-4534-b54a-f7ae05159d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "mymodel.summary()#Display out the blueprint"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 600)               29400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 7)                 4207      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 120)               1080      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 7)                 847       \n",
            "=================================================================\n",
            "Total params: 35,534\n",
            "Trainable params: 35,534\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB6hnLS8nLWJ",
        "outputId": "6e24f5d2-305f-421e-91ad-1b7b30a7cd33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#Create the model turned using the above helper\n",
        "def myturned_nnt(dropout, lr, neuronsRate,neuronsCut):\n",
        "  SPLITS = 10\n",
        "  boots_samples = StratifiedShuffleSplit(n_splits = SPLITS, test_size = 0.1)\n",
        "  average_log_loss = []\n",
        "  average_epoch = []\n",
        "  count = 0\n",
        "  neurons_nums = int(neuronsRate * 8000)\n",
        "  #Looping through the bootstrap samples:\n",
        "  for train, test in boots_samples.split(x, y):\n",
        "    time_start = time.time()\n",
        "    count+=1\n",
        "    x_train = x[train]\n",
        "    x_test = x[test]\n",
        "    y_train = y[train]\n",
        "    y_test = y[test]\n",
        "\n",
        "    #Generating the model using the above helper function\n",
        "    model = helper_hyper(dropout, neuronsRate, neuronsCut)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr=lr))\n",
        "    info = EarlyStopping(monitor = 'val_loss', patience = 100, verbose = 1, restore_best_weights = True, min_delta = 1e-3)\n",
        "    model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 1000, verbose = 0, callbacks = [info])\n",
        "    epoc1 = info.stopped_epoch\n",
        "    average_epoch.append(epoc1)\n",
        "    pred = model.predict(x_test)\n",
        "    y_match = np.argmax(y_test, axis = 1)\n",
        "    scoreloss = metrics.log_loss(y_match, pred)\n",
        "    average_log_loss.append(scoreloss)\n",
        "    time_end = time.time()\n",
        "    time_elapsed = time_start - time_end\n",
        "  tensorflow.keras.backend.clear_session()\n",
        "  return f\"Average_logloss: {statistics.mean(average_log_loss)}\\n Average epochs: {statistics.mean(average_epoch)}\\n Time elapsed: {time_elapsed}\"\n",
        "\n",
        "print(myturned_nnt(dropout = 0.2, lr = 1e-03, neuronsRate = 0.1, neuronsCut = 0.25))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00133: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00125: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00130: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00117: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00116: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00115: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00121: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00130: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00112: early stopping\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00127: early stopping\n",
            "Average_logloss: 0.7259871798576787\n",
            " Average epochs: 121.6\n",
            " Time elapsed: -16.94886016845703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLSMuQx8tAr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}