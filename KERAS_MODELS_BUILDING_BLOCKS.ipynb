{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KERAS MODELS-BUILDING BLOCKS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVzfxg2YiJbk7U51nKxuRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Applied-Predictive-Modeling2/blob/master/KERAS_MODELS_BUILDING_BLOCKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t2tIaf688hx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCO1EWVw9In-",
        "outputId": "08c587c1-c326-4673-c971-67a8d7960f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Building keras layers from scratch\n",
        "class MyLayer(keras.layers.Layer):\n",
        "  def __init__(self, units = 64, input_shape=64):\n",
        "    super(MyLayer, self).__init__()\n",
        "    w_initial = tf.random_normal_initializer()\n",
        "    b_initial = tf.zeros_initializer()\n",
        "    self.w = tf.Variable(initial_value = w_initial(shape = (units, input_shape)), dtype = 'float32', trainable = True)\n",
        "    self.b = tf.Variable(initial_value = b_initial(shape = (units, )), dtype = 'float32', trainable = True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "x = tf.ones(shape = (3,3))\n",
        "layer1 = MyLayer(3,1)\n",
        "out = layer1(x)\n",
        "print(out)\n",
        "assert layer1.weights == [layer1.w, layer1.b]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.10312843 -0.10312843 -0.10312843]\n",
            " [-0.10312843 -0.10312843 -0.10312843]\n",
            " [-0.10312843 -0.10312843 -0.10312843]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T911rUBL_82u",
        "outputId": "4d3246e0-879a-442b-f88f-597d81159ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Altenative way to add parameters in a keras layer\n",
        "class AltLayer(keras.layers.Layer):\n",
        "  def __init__(self, input_shape = 64, units = 64):\n",
        "    super(AltLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape = (input_shape, units), initializer = 'random_normal',trainable = True)\n",
        "    self.b = self.add_weight(shape = (units,), initializer = 'zeros', trainable = True)\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "x = tf.ones(shape = (3,3))\n",
        "layer2 = AltLayer(3,1)\n",
        "out = layer2(x)\n",
        "print(out)\n",
        "assert layer2.weights == [layer2.w, layer2.b]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.02011857]\n",
            " [0.02011857]\n",
            " [0.02011857]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zLPT7R9HpHw",
        "outputId": "790ec00b-3a70-4056-ce12-15ea2ac36659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Adding the input shape in a separate block\n",
        "class AddInput(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(AddInput, self).__init__()\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        shape = (input_shape[-1], self.units),\n",
        "        initializer = 'random_normal',\n",
        "        trainable = True\n",
        "    )\n",
        "    self.b = self.add_weight(\n",
        "        shape = (self.units,),\n",
        "        initializer = 'zeros',\n",
        "        trainable = True\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "layer3 = AddInput(32)\n",
        "out = layer3(x)\n",
        "print(out)\n",
        "assert layer3.weights == [layer3.w, layer3.b]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.03820161 -0.00713975 -0.14460814  0.01829805  0.06159644  0.10677131\n",
            "   0.0763822  -0.05080696 -0.03154647 -0.09491213 -0.03052873 -0.05248065\n",
            "  -0.08826051  0.0778743  -0.09496681  0.06347735  0.01456482 -0.0052099\n",
            "  -0.07416502 -0.05922581  0.05194514  0.06442119 -0.07996801 -0.02404549\n",
            "   0.02627587 -0.10059783  0.06422863  0.07928954 -0.07785238  0.10868542\n",
            "  -0.10411608  0.07014898]\n",
            " [ 0.03820161 -0.00713975 -0.14460814  0.01829805  0.06159644  0.10677131\n",
            "   0.0763822  -0.05080696 -0.03154647 -0.09491213 -0.03052873 -0.05248065\n",
            "  -0.08826051  0.0778743  -0.09496681  0.06347735  0.01456482 -0.0052099\n",
            "  -0.07416502 -0.05922581  0.05194514  0.06442119 -0.07996801 -0.02404549\n",
            "   0.02627587 -0.10059783  0.06422863  0.07928954 -0.07785238  0.10868542\n",
            "  -0.10411608  0.07014898]\n",
            " [ 0.03820161 -0.00713975 -0.14460814  0.01829805  0.06159644  0.10677131\n",
            "   0.0763822  -0.05080696 -0.03154647 -0.09491213 -0.03052873 -0.05248065\n",
            "  -0.08826051  0.0778743  -0.09496681  0.06347735  0.01456482 -0.0052099\n",
            "  -0.07416502 -0.05922581  0.05194514  0.06442119 -0.07996801 -0.02404549\n",
            "   0.02627587 -0.10059783  0.06422863  0.07928954 -0.07785238  0.10868542\n",
            "  -0.10411608  0.07014898]], shape=(3, 32), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "splum4XlLSOw",
        "outputId": "ddf4bdc3-5d40-419f-a24c-08d9b0c08a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Stacking keras layers together\n",
        "class StackLayer(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(StackLayer, self).__init__()\n",
        "    self.ln1 = AddInput(32)\n",
        "    self.ln2 = AddInput(16)\n",
        "    self.ln3 = AddInput(1)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    x = self.ln1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.ln2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return self.ln3(x)\n",
        "  \n",
        "model= StackLayer()\n",
        "model(tf.ones(shape = (3,32)))\n",
        "print(f\"trainable pars: {len(model.weights)}\\nNon-trainable pars: {len(model.trainable_weights)}\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainable pars: 6\n",
            "Non-trainable pars: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr_sjrS1VoD6"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZH7VCZXQSpj"
      },
      "source": [
        "#Add Loss as a regularizer to the keras layer( This can be added as additional loss to the main loss)\n",
        "class AddLoss(keras.layers.Layer):\n",
        "  def __init__(self,rate = 1e-3):\n",
        "    super(AddLoss, self).__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "    return inputs\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWd6t1W2WJ1u"
      },
      "source": [
        "#retrieve the loss at any stage using layer.losses\n",
        "class LossRetrieve(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(LossRetrieve, self).__init__()\n",
        "    self.myregulator = AddLoss(1e-3)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return self.myregulator(inputs)\n",
        "\n",
        "\n",
        "los1 = LossRetrieve()\n",
        "_=los1(tf.zeros((1,1)))\n",
        "assert len(los1.losses) == 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akbFY0MaWaDn",
        "outputId": "41833918-11ce-42cd-e53f-65b476ce9ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Adding kernel regularizer to the layer\n",
        "class WithKernel(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(WithKernel, self).__init__()\n",
        "    self.mydense = keras.layers.Dense(units = 32,\n",
        "                                      kernel_regularizer = tf.keras.regularizers.l2(1e-3))\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return self.mydense(inputs)\n",
        "\n",
        "ken = WithKernel()\n",
        "_= ken(tf.zeros((1,1)))\n",
        "print(ken.losses)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0018590672>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfw57wbGgc0l",
        "outputId": "5960fa10-85a7-42d7-afc5-43c5d3a0d5e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Using the above defined losses (loss, regularizer to train keras model)\n",
        "inputs = keras.Input(shape = (3,))\n",
        "outputs = AddLoss()(inputs)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "model.compile(loss = 'mse', optimizer = 'adam')#with the main loss also user defined losses are added\n",
        "model.summary()\n",
        "model.fit(np.random.random((64,3)), np.random.random((64,3)))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "add_loss_1 (AddLoss)         (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.1908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd4827abb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_6_UUlri-Nx",
        "outputId": "11f271c6-4179-4abb-aaf9-d3a147e8ab99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Training using only the user defined loss\n",
        "inputs = keras.Input(shape = (32,))\n",
        "outputs = AddLoss()(inputs)\n",
        "model1 = keras.Model(inputs = inputs, outputs = outputs)\n",
        "model1.compile(optimizer = 'adam')\n",
        "model1.fit(np.random.random(size = (64,32)), np.random.random(size = (64,32)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step - loss: 0.5050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd481ed5828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfExFuAikMS8"
      },
      "source": [
        "#Adding a metric to the keras model to track moving average of a quantity during training\n",
        "#This compute the accuracy of the model\n",
        "class LogisticLoss(keras.layers.Layer):\n",
        "  def __init__(self, name = None):\n",
        "    super(LogisticLoss, self).__init__(name = name)\n",
        "    self.logloss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    self.accuracy1 = keras.metrics.BinaryAccuracy()\n",
        "  \n",
        "  def call(self, y_hat, y_orig, sample_inputs = None):\n",
        "    loss1 = self.logloss(y_hat, y_orig, sample_inputs)\n",
        "    self.add_loss(loss1)\n",
        "    accuracy2 = self.accuracy1(y_hat, y_orig, sample_inputs)\n",
        "    self.add_metric(accuracy2, name = 'accuracy')\n",
        "    return tf.nn.softmax(logits=y_hat)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039O9wGt23Cx",
        "outputId": "148ccb77-8597-4260-eb55-2fed95fd5ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mymetrics = LogisticLoss()\n",
        "y_hat = tf.ones(shape = (3,3))\n",
        "y_orig = tf.ones(shape = (3,3))\n",
        "out_loss = mymetrics(y_hat, y_orig)\n",
        "print(f\"Layer_metric: {mymetrics.metrics}\\ncurrent accuracy: {float(mymetrics.metrics[0].result())}\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer_metric: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7fd480687240>]\n",
            "current accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z4svhfw32Je",
        "outputId": "459dd490-cbd1-45a7-b2b9-957125cc1a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#We can track this metric using fit() like in loss fn\n",
        "inputs = keras.Input(shape = (3,), name = 'input')\n",
        "y_orig = keras.Input(shape = (10,), name = 'target')\n",
        "y_hat = keras.layers.Dense(units = 10)(inputs)\n",
        "pred = LogisticLoss(name = 'predictions')(y_hat, y_orig)\n",
        "model = keras.Model(inputs = [inputs, y_orig], outputs = pred)\n",
        "model.compile(optimizer = 'adam')\n",
        "dfm = {\n",
        "    'inputs':np.random.random(size = (10,3)),\n",
        "    'y_orig':np.random.random(size = (10,10))\n",
        "}\n",
        "\n",
        "model.fit(dfm)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0233 - binary_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd489590ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96dgwTwXKqyx",
        "outputId": "04a3f818-5300-41dd-9543-7c47b942b265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Serialize the hard coded layers as ussual keras functional API models using get_config\n",
        "\n",
        "class SerializedLayers(keras.layers.Layer):\n",
        "  def __init__(self, units = 64):\n",
        "    super(SerializedLayers, self).__init__()\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        shape =(input_shape[-1], self.units),\n",
        "        initializer = 'random_normal', \n",
        "        trainable = True)\n",
        "    \n",
        "    self.b = self.add_weight(\n",
        "        shape = (self.units,),\n",
        "        initializer = 'zeros',\n",
        "        trainable = True)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) +self.b\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {'units': self.units}\n",
        "    \n",
        "layer = SerializedLayers(32)\n",
        "out = layer(x)\n",
        "print(out)\n",
        "assert layer.weights == [layer.w, layer.b]\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "stack_layer = SerializedLayers.from_config(config)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.1100989   0.00880036 -0.0440092   0.07522993  0.09891136  0.00375583\n",
            "   0.06353734  0.00867078  0.04658917  0.05582516  0.02994575 -0.04943225\n",
            "   0.04391855  0.07978    -0.08174905  0.08592284 -0.08341957 -0.0199663\n",
            "  -0.09680377  0.00232024  0.01505715 -0.00750295 -0.02942482 -0.0687234\n",
            "  -0.03114662  0.06874943 -0.05522085 -0.04717303 -0.00889447 -0.10763291\n",
            "  -0.05303584  0.07200257]\n",
            " [ 0.1100989   0.00880036 -0.0440092   0.07522993  0.09891136  0.00375583\n",
            "   0.06353734  0.00867078  0.04658917  0.05582516  0.02994575 -0.04943225\n",
            "   0.04391855  0.07978    -0.08174905  0.08592284 -0.08341957 -0.0199663\n",
            "  -0.09680377  0.00232024  0.01505715 -0.00750295 -0.02942482 -0.0687234\n",
            "  -0.03114662  0.06874943 -0.05522085 -0.04717303 -0.00889447 -0.10763291\n",
            "  -0.05303584  0.07200257]\n",
            " [ 0.1100989   0.00880036 -0.0440092   0.07522993  0.09891136  0.00375583\n",
            "   0.06353734  0.00867078  0.04658917  0.05582516  0.02994575 -0.04943225\n",
            "   0.04391855  0.07978    -0.08174905  0.08592284 -0.08341957 -0.0199663\n",
            "  -0.09680377  0.00232024  0.01505715 -0.00750295 -0.02942482 -0.0687234\n",
            "  -0.03114662  0.06874943 -0.05522085 -0.04717303 -0.00889447 -0.10763291\n",
            "  -0.05303584  0.07200257]], shape=(3, 32), dtype=float32)\n",
            "{'units': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OfVnDw5Xtwz"
      },
      "source": [
        "#Customize some keras layers such as BatchNorm and Dropout!These layers are \n",
        "#very important in regulating the accuracy of the model\n",
        "\n",
        "#make privelage for the training argument in a call method\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "  def __init__(self, my_rate,**kwargs):\n",
        "    super(CustomLayer, self).__init__(**kwargs)\n",
        "    self.my_rate = my_rate\n",
        "  \n",
        "  def call(self, inputs, training = None):\n",
        "    if training:\n",
        "      tf.nn.dropout(inputs, rate = self.my_rate)\n",
        "      return inputs\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4IxeLC9o8uP"
      },
      "source": [
        "#Training VAE end to end from scratch\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkY5SdV5cUbo"
      },
      "source": [
        "#Sampling from the compressed layer (output of the encoder)\n",
        "class Mysample(keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mean_z, logvar_z = inputs\n",
        "    eps = tf.keras.backend.random_normal(shape = (tf.shape(mean_z)[0], tf.shape(mean_z)[1]))\n",
        "    return mean_z + tf.exp(0.5*logvar_z)*eps #Assumption of a std Gaussian for the decorder's input\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahhH9Sv-hC5N"
      },
      "source": [
        "#The encoder's class: This takes the input and compress into hidden representation\n",
        "class MyEncoder(keras.layers.Layer):\n",
        "  def __init__(self,latent1 = 64, latent2 = 32,name = 'Encoder', **kwargs):\n",
        "    super(MyEncoder, self).__init__(name = name, **kwargs)\n",
        "    self.l1 = keras.layers.Dense(units = latent1,activation = 'relu')\n",
        "    self.l2 = keras.layers.Dense(units = latent2, activation = 'relu')#For the mean_z\n",
        "    self.l3 = keras.layers.Dense(units = latent2, activation = 'relu')#For the logvar_z\n",
        "    self.mysample = Mysample()#For the z's\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.l1(inputs)\n",
        "    mean_z = self.l2(x)\n",
        "    logvar_z = self.l3(x)\n",
        "    z = self.mysample((mean_z, logvar_z))\n",
        "    return mean_z, logvar_z, z\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsShKlbnkZK2"
      },
      "source": [
        "#Reconstruct the original data--Moving from compressed to readable images\n",
        "class MyDecoder(keras.layers.Layer):\n",
        "  def __init__(self,dim_orig, latent1 = 64,name = 'Decoder',**kwargs):\n",
        "    super(MyDecoder,self).__init__(name = name,**kwargs)\n",
        "    self.d1 = keras.layers.Dense(units = latent1, activation = 'relu')#starting expanding the encoded layer\n",
        "    self.d2 = keras.layers.Dense(units = dim_orig, activation = 'sigmoid')#Reconstruct the original data\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.d1(inputs)\n",
        "    return self.d2(x)\n",
        "\n",
        "#Combining both classes and train together from end to end\n",
        "class VarAutoEncoder(keras.Model):\n",
        "  def __init__(\n",
        "      self,\n",
        "      dim_orig,\n",
        "      latent1 = 64,\n",
        "      latent2 = 32,name = 'autoencoder',**kwargs):\n",
        "    super(VarAutoEncoder, self).__init__(name = name, **kwargs)\n",
        "    self.dim_orig = dim_orig\n",
        "    self.encoder = MyEncoder(latent1 = latent2, latent2=latent1)\n",
        "    self.decoder = MyDecoder(dim_orig = dim_orig, latent1 = latent1)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    mean_z, logvar_z, z = self.encoder(inputs)#Run the encoder\n",
        "    reconstruction = self.decoder(z)\n",
        "    KL_div = -0.5*tf.reduce_mean(\n",
        "        logvar_z -tf.square(mean_z) - tf.exp(logvar_z) +1\n",
        "    )\n",
        "    self.add_loss(KL_div) #As regularization to assure randomness in mapping z to original x\n",
        "    return reconstruction\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vImFbu-xawQ",
        "outputId": "bdc7b58e-9105-4b48-bbe4-706aff1e0742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#A training loop\n",
        "dim_orig = 784\n",
        "V_model = VarAutoEncoder(dim_orig = dim_orig,latent1 = 64, latent2 = 32)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "mse = keras.losses.MeanSquaredError()\n",
        "metric_L = keras.metrics.Mean()\n",
        "\n",
        "#Loading the dataset\n",
        "(x_train,_),_= keras.datasets.mnist.load_data()\n",
        "#Reshape and preprocess sampled trainining data\n",
        "x_train = x_train.reshape(60000, 784).astype('float32')/255\n",
        "train_dfm = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "#Shuffle the data and select the batches of size 64\n",
        "train_dfm = train_dfm.shuffle(buffer_size = 1024).batch(64)\n",
        "\n",
        "#Iterate over # of epochs\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d'%(epoch,))\n",
        "\n",
        "  for step, train_batch in enumerate(train_dfm):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstruction = V_model(train_batch)\n",
        "      myloss = mse(train_batch, reconstruction)\n",
        "      myloss+=sum(V_model.losses)\n",
        "      grads = tape.gradient(myloss, V_model.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(grads,V_model.trainable_weights))\n",
        "\n",
        "      metric_L(myloss)\n",
        "      if step % 100 == 0:\n",
        "        print(\"step %d: mean loss = %.4f\" % (step, metric_L.result()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n",
            "step 0: mean loss = 0.2608\n",
            "step 100: mean loss = 0.1175\n",
            "step 200: mean loss = 0.0948\n",
            "step 300: mean loss = 0.0862\n",
            "step 400: mean loss = 0.0820\n",
            "step 500: mean loss = 0.0791\n",
            "step 600: mean loss = 0.0773\n",
            "step 700: mean loss = 0.0759\n",
            "step 800: mean loss = 0.0749\n",
            "step 900: mean loss = 0.0739\n",
            "Start of epoch 1\n",
            "step 0: mean loss = 0.0737\n",
            "step 100: mean loss = 0.0731\n",
            "step 200: mean loss = 0.0727\n",
            "step 300: mean loss = 0.0723\n",
            "step 400: mean loss = 0.0720\n",
            "step 500: mean loss = 0.0717\n",
            "step 600: mean loss = 0.0714\n",
            "step 700: mean loss = 0.0711\n",
            "step 800: mean loss = 0.0709\n",
            "step 900: mean loss = 0.0707\n",
            "Start of epoch 2\n",
            "step 0: mean loss = 0.0706\n",
            "step 100: mean loss = 0.0705\n",
            "step 200: mean loss = 0.0704\n",
            "step 300: mean loss = 0.0703\n",
            "step 400: mean loss = 0.0702\n",
            "step 500: mean loss = 0.0700\n",
            "step 600: mean loss = 0.0699\n",
            "step 700: mean loss = 0.0698\n",
            "step 800: mean loss = 0.0697\n",
            "step 900: mean loss = 0.0696\n",
            "Start of epoch 3\n",
            "step 0: mean loss = 0.0696\n",
            "step 100: mean loss = 0.0695\n",
            "step 200: mean loss = 0.0695\n",
            "step 300: mean loss = 0.0694\n",
            "step 400: mean loss = 0.0694\n",
            "step 500: mean loss = 0.0693\n",
            "step 600: mean loss = 0.0693\n",
            "step 700: mean loss = 0.0692\n",
            "step 800: mean loss = 0.0692\n",
            "step 900: mean loss = 0.0691\n",
            "Start of epoch 4\n",
            "step 0: mean loss = 0.0691\n",
            "step 100: mean loss = 0.0690\n",
            "step 200: mean loss = 0.0690\n",
            "step 300: mean loss = 0.0690\n",
            "step 400: mean loss = 0.0690\n",
            "step 500: mean loss = 0.0689\n",
            "step 600: mean loss = 0.0689\n",
            "step 700: mean loss = 0.0688\n",
            "step 800: mean loss = 0.0688\n",
            "step 900: mean loss = 0.0688\n",
            "Start of epoch 5\n",
            "step 0: mean loss = 0.0687\n",
            "step 100: mean loss = 0.0687\n",
            "step 200: mean loss = 0.0687\n",
            "step 300: mean loss = 0.0687\n",
            "step 400: mean loss = 0.0687\n",
            "step 500: mean loss = 0.0686\n",
            "step 600: mean loss = 0.0686\n",
            "step 700: mean loss = 0.0686\n",
            "step 800: mean loss = 0.0686\n",
            "step 900: mean loss = 0.0685\n",
            "Start of epoch 6\n",
            "step 0: mean loss = 0.0685\n",
            "step 100: mean loss = 0.0685\n",
            "step 200: mean loss = 0.0685\n",
            "step 300: mean loss = 0.0685\n",
            "step 400: mean loss = 0.0685\n",
            "step 500: mean loss = 0.0685\n",
            "step 600: mean loss = 0.0684\n",
            "step 700: mean loss = 0.0684\n",
            "step 800: mean loss = 0.0684\n",
            "step 900: mean loss = 0.0684\n",
            "Start of epoch 7\n",
            "step 0: mean loss = 0.0684\n",
            "step 100: mean loss = 0.0684\n",
            "step 200: mean loss = 0.0684\n",
            "step 300: mean loss = 0.0683\n",
            "step 400: mean loss = 0.0683\n",
            "step 500: mean loss = 0.0683\n",
            "step 600: mean loss = 0.0683\n",
            "step 700: mean loss = 0.0683\n",
            "step 800: mean loss = 0.0683\n",
            "step 900: mean loss = 0.0683\n",
            "Start of epoch 8\n",
            "step 0: mean loss = 0.0683\n",
            "step 100: mean loss = 0.0682\n",
            "step 200: mean loss = 0.0682\n",
            "step 300: mean loss = 0.0682\n",
            "step 400: mean loss = 0.0682\n",
            "step 500: mean loss = 0.0682\n",
            "step 600: mean loss = 0.0682\n",
            "step 700: mean loss = 0.0682\n",
            "step 800: mean loss = 0.0682\n",
            "step 900: mean loss = 0.0682\n",
            "Start of epoch 9\n",
            "step 0: mean loss = 0.0682\n",
            "step 100: mean loss = 0.0682\n",
            "step 200: mean loss = 0.0681\n",
            "step 300: mean loss = 0.0681\n",
            "step 400: mean loss = 0.0681\n",
            "step 500: mean loss = 0.0681\n",
            "step 600: mean loss = 0.0681\n",
            "step 700: mean loss = 0.0681\n",
            "step 800: mean loss = 0.0681\n",
            "step 900: mean loss = 0.0681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE2gxlio4HZ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}